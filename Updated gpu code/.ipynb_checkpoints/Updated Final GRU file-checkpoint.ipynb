{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('fivethirtyeight')\n",
    "from pylab import rcParams\n",
    "from plotly import tools\n",
    "#import plotly.plotly as py\n",
    "import chart_studio.plotly as py\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "# init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "import statsmodels.api as sm\n",
    "from numpy.random import normal, seed\n",
    "from scipy.stats import norm\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.arima_process import ArmaProcess\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#print(os.listdir(\"../input\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe0 = pd.read_csv(\"./train.csv\")\n",
    "#dataframe1 = pd.read_csv('./building_1_anolomy.csv')\n",
    "dataframe1 = pd.read_csv('./building_2_anolomy.csv')\n",
    "#dataframe3 = pd.read_csv('./building_3_anolomy.csv')\n",
    "#dataframe4 = pd.read_csv('./building_4_anolomy.csv')\n",
    "#dataframe5 = pd.read_csv('./building_5_anolomy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of raw data is -:  (6600, 23)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6600 entries, 0 to 6599\n",
      "Data columns (total 22 columns):\n",
      "timestamp            6600 non-null object\n",
      "timestamp.1          6600 non-null object\n",
      "main_meter           6600 non-null float64\n",
      "sub_meter_1          6600 non-null float64\n",
      "sub_meter_2          6600 non-null float64\n",
      "weekend              6600 non-null int64\n",
      "corporate            6600 non-null int64\n",
      "main_meter_4         6600 non-null float64\n",
      "main_meter_12        6600 non-null float64\n",
      "day of week_0        6600 non-null int64\n",
      "day of week_1        6600 non-null int64\n",
      "day of week_2        6600 non-null int64\n",
      "day of week_3        6600 non-null int64\n",
      "day of week_4        6600 non-null int64\n",
      "day of week_5        6600 non-null int64\n",
      "day of week_6        6600 non-null int64\n",
      "building_number_1    6600 non-null int64\n",
      "building_number_2    6600 non-null int64\n",
      "building_number_3    6600 non-null int64\n",
      "building_number_4    6600 non-null int64\n",
      "building_number_5    6600 non-null int64\n",
      "building_number      6600 non-null int64\n",
      "dtypes: float64(5), int64(15), object(2)\n",
      "memory usage: 1.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "interval = [dataframe1.shape[0]]#, dataframe2.shape[0], dataframe3.shape[0], dataframe4.shape[0], dataframe5.shape[0]]\n",
    "\n",
    "dataframe = dataframe1#pd.concat([dataframe1, dataframe2, dataframe3, dataframe4, dataframe5], axis = 0)\n",
    "print(\"The shape of raw data is -: \", dataframe.shape)\n",
    "\n",
    "\n",
    "dataframe = dataframe.drop(['Unnamed: 0'],axis=1)\n",
    "print(dataframe.info())\n",
    "dataframe['timestamp']=pd.to_datetime(dataframe['timestamp'],format='%d-%m-%Y %H:%M')\n",
    "dataframe0['timestamp']=pd.to_datetime(dataframe0['timestamp'],format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "dataframe.set_index(dataframe['timestamp'],inplace=True)\n",
    "dataframe = dataframe.drop(['timestamp'],axis=1)\n",
    "dataframe.reset_index(inplace=True)\n",
    "\n",
    "dataframe0.set_index(dataframe0['timestamp'],inplace=True)\n",
    "dataframe0 = dataframe0.drop(['timestamp'],axis=1)\n",
    "dataframe0.reset_index(inplace=True)\n",
    "\n",
    "dataframe0.index = dataframe0['timestamp']\n",
    "dataframe.index = dataframe['timestamp']\n",
    "time_series = dataframe[['main_meter', 'sub_meter_1', 'sub_meter_2', 'corporate', 'day of week_0','day of week_1','day of week_2','day of week_3','day of week_4','day of week_5','day of week_6']]#,'building_number_1','building_number_2','building_number_3','building_number_4','building_number_5']] #'building_number','Hour', 'day of week']]#\n",
    "\n",
    "dummy_test_data = dataframe0[['main_meter','sub_meter_1', 'sub_meter_2']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import newaxis\n",
    "from keras.layers.core import Dense ,Activation,Dropout\n",
    "from keras.layers.recurrent import LSTM,GRU\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "##LSTM forecasting for the data\n",
    "\n",
    "val_time = 1\n",
    "n_input = 72\n",
    "n_features = 11\n",
    "val_length = int(float(dataframe1.shape[0])*0.2)\n",
    "n_pred = 3\n",
    "\n",
    "val_gap = val_time*4\n",
    "\n",
    "sum = 0\n",
    "#splitting into train and test data\n",
    "for i, c in enumerate(interval):\n",
    "\tprint(i)\n",
    "\tif (i == 0):\n",
    "\t\ttrain_data = time_series[:c-val_length]\n",
    "\t\ttest_data = time_series[c-val_length:c]\n",
    "\t\tactual_test_data = dummy_test_data[(c-val_length)*val_gap:c*val_gap]\n",
    "\telse:\n",
    "\t\ttrain_data = pd.concat([train_data, time_series[sum:sum+c-val_length]])\n",
    "\t\ttest_data = pd.concat([test_data, time_series[sum+c-val_length:sum+c]])\n",
    "\t\tactual_test_data = pd.concat([actual_test_data, dummy_test_data[(sum+c-val_length)*val_gap:(sum+c)*val_gap]])\n",
    "\tsum += c\n",
    "\tinterval[i] = sum - (i+1)*val_length\n",
    "\n",
    "test_data = pd.DataFrame(test_data).values.reshape(-1,n_features)\n",
    "train_data = pd.DataFrame(train_data).values.reshape(-1,n_features)\n",
    "df_test = actual_test_data.copy()\n",
    "actual_test_data = actual_test_data.to_numpy()\n",
    "actual_test_data = actual_test_data.reshape(-1, n_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data after final processing -:  (5208, 72, 11)\n"
     ]
    }
   ],
   "source": [
    "#starting with intialization \n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit(time_series)\n",
    "scaled_train_data = scaler.transform(train_data)\n",
    "scaled_test_data = scaler.transform(test_data)\n",
    "\n",
    "\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for j, c in enumerate(interval):\n",
    "\tif (j==0):\n",
    "\t\t# print(n_input)\n",
    "\t\tfor i in range(n_input, c):\n",
    "\t\t\tX_train.append(scaled_train_data[i-n_input:i, :])\n",
    "\t\t\ty_train.append(scaled_train_data[i, :n_pred])\n",
    "\telse:\n",
    "\t\t# print(interval[j-1]+n_input)\n",
    "\t\tfor i in range(interval[j-1]+n_input, c):\n",
    "\t\t\tX_train.append(scaled_train_data[i-n_input:i, :])\n",
    "\t\t\ty_train.append(scaled_train_data[i, :n_pred])\n",
    "\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], n_input, n_features))\n",
    "\n",
    "perm = np.random.permutation(X_train.shape[0])\n",
    "X_train = X_train[perm]\n",
    "y_train = y_train[perm]\n",
    "\n",
    "print(\"Shape of training data after final processing -: \", X_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru (GRU)                    (None, 72, 300)           281700    \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 50)                52800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3)                 153       \n",
      "=================================================================\n",
      "Total params: 334,653\n",
      "Trainable params: 334,653\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "val = \"building2_1hr_sigmoid_3meter_adam_1\"\n",
    "'''\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(GRU(300,  return_sequences=True, activation='relu',input_shape=(n_input,n_features)))\n",
    "lstm_model.add(GRU(50, return_sequences=True, activation='relu'))\n",
    "lstm_model.add(GRU(50,activation='relu'))\n",
    "lstm_model.add(Dense(3, activation='sigmoid'))\n",
    "\n",
    "# sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "lstm_model.compile(optimizer='adam',loss='mse')\n",
    "\n",
    "print(lstm_model.summary())\n",
    "\n",
    "model_json = lstm_model.to_json()\n",
    "with open(\"./Json/model_LSTM_model_all_\"+ val +\".json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "\n",
    "lstm_model.fit(X_train, y_train, epochs=40, batch_size = 1024)\n",
    "\n",
    "lstm_model.save_weights(\"./weights/model_LSTM_model_all_\"+ val +\".h5\")\n",
    "print(\"Saved model to disk\")\n",
    "'''\n",
    "json_file = open(\"./Json/model_LSTM_model_all_\"+ val +\".json\", 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "lstm_model = model_from_json(loaded_model_json)\n",
    "print(lstm_model.summary())\n",
    "# load weights into new model\n",
    "lstm_model.load_weights(\"./weights/model_LSTM_model_all_\"+ val +\".h5\")\n",
    "print(\"Loaded model from disk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5280, 3)\n",
      "(5280, 3)\n",
      "(5280, 3) (5280, 3)\n",
      "########################### For Building 0###########################\n",
      "METER 1-: \n",
      "1518.5444898626743\n",
      "(1320,)\n",
      "11.782478890086823\n",
      "METER 2-: \n",
      "410.0394021116729\n",
      "(1320,)\n",
      "11.580950572343186\n",
      "METER 3-: \n",
      "426.5966503311798\n",
      "(1320,)\n",
      "42.52513007650098\n"
     ]
    }
   ],
   "source": [
    "lstm_predictions_scaled = list()\n",
    "\n",
    "sum = 0\n",
    "for c in interval:\n",
    "\tbatch = scaled_train_data[c-n_input:c, :]\n",
    "\tcurrent_batch = batch.reshape((1,n_input,n_features))\n",
    "\tfor i in range(sum, sum+val_length):   \n",
    "\t\t# print((lstm_model.predict(current_batch)).astype(np.float64))\n",
    "\t\tlstm_pred = ((lstm_model.predict(current_batch)).astype(np.float64))\n",
    "\t\t# print(train_data[c-n_input+i-sum,1], test_data[i][1])\n",
    "\t\t#print(lstm_pred, scaled_test_data[i][0], test_data[i][0])\n",
    "\t\tdummy = test_data[i].copy()\n",
    "\t\tdummy[:n_pred] = lstm_pred\n",
    "\t\tlstm_predictions_scaled.append(dummy)\n",
    "\t\tcurrent_batch = np.append(current_batch[:,1:],[dummy.reshape(1, -1)], axis=1)\n",
    "\tsum += val_length\n",
    "\n",
    "lstm_predictions = scaler.inverse_transform(lstm_predictions_scaled)\n",
    "pred = np.array(lstm_predictions[:, :n_pred])\n",
    "pred = np.repeat(pred, val_gap, axis = 0)\n",
    "print(pred.shape)\n",
    "# exit()\n",
    "print(actual_test_data.shape)\n",
    "pred = pred.reshape(-1, n_pred)\n",
    "# for i in range(pred.shape[0]):\n",
    "# \tprint(pred[i][0], actual_test_data[i][0])\n",
    "print(pred.shape, actual_test_data.shape)\n",
    "\n",
    "def evalution_metric(m,m_hat,timestamp):\n",
    "    Dt=timestamp.day\n",
    "    Dt = Dt.to_numpy()\n",
    "    # Dt = np.repeat(Dt, val_gap)\n",
    "    print(Dt.shape)\n",
    "    Sum=0\n",
    "    for i in range(len(m)):\n",
    "        Sum+=np.power((m[i]-m_hat[i]),2)*np.exp(-(np.log(2)/100)*Dt[i])\n",
    "    score=(1/np.mean(m))*(np.sqrt(Sum))\n",
    "    return score\n",
    "\n",
    "# #calculating metrics\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import mean_squared_log_error as msle\n",
    "\n",
    "sum = 0\n",
    "pred_main_meter1_building2_try = pred[sum:sum+val_length,0]\n",
    "for j, c in enumerate(interval):\n",
    "\tprint(\"########################### For Building \"+str(j)+\"###########################\")\t\n",
    "\tfor i in range(n_pred):\n",
    "\t\tmean_squared_error_lstm = mse(actual_test_data[sum:sum+val_length,i], pred[sum:sum+val_length,i])\n",
    "\t\t# mean_squared_log_error_lstm = msle(test_data,lstm_predictions)\n",
    "\t\tprint(\"METER \"+str(i+1)+\"-: \")\n",
    "\t\tprint(mean_squared_error_lstm**0.5)\n",
    "\t\tprint(evalution_metric(actual_test_data[sum:sum+val_length,i],pred[sum:sum+val_length, i],df_test[sum:sum+val_length].index))\n",
    "\tsum += val_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
